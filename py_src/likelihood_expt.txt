19:04 bilby INFO    : Running for label 'likelihood_expt_ozstar', output will be saved to '../data/nested_sampling/'
19:04 bilby INFO    : Analysis priors:
19:04 bilby INFO    : omega_gw=LogUniform(minimum=1e-09, maximum=1e-05, name='omega_gw', latex_label='omega_gw', unit=None, boundary=None)
19:04 bilby INFO    : h=LogUniform(minimum=0.0001, maximum=1.0, name='h', latex_label='h', unit=None, boundary=None)
19:04 bilby INFO    : phi0_gw=0.2
19:04 bilby INFO    : psi_gw=2.5
19:04 bilby INFO    : iota_gw=1.0
19:04 bilby INFO    : delta_gw=1.0
19:04 bilby INFO    : alpha_gw=1.0
19:04 bilby INFO    : f00=327.8470205611185
19:04 bilby INFO    : f01=205.53069910059
19:04 bilby INFO    : f02=303.0909794113198
19:04 bilby INFO    : f03=326.600561967271
19:04 bilby INFO    : f04=348.5592316999902
19:04 bilby INFO    : f05=112.94972319066429
19:04 bilby INFO    : f06=346.5319964932129
19:04 bilby INFO    : f07=215.6088104676294
19:04 bilby INFO    : f08=190.2678373613727
19:04 bilby INFO    : f09=193.7156834116254
19:04 bilby INFO    : f010=238.004053174088
19:04 bilby INFO    : f011=172.642909924795
19:04 bilby INFO    : f012=125.2002451675204
19:04 bilby INFO    : f013=277.9377069492822
19:04 bilby INFO    : f014=317.3789419337929
19:04 bilby INFO    : f015=316.12398422451724
19:04 bilby INFO    : f016=216.3733370950632
19:04 bilby INFO    : f017=218.811840394717
19:04 bilby INFO    : f018=170.93736991146392
19:04 bilby INFO    : f019=266.8691669720664
19:04 bilby INFO    : f020=245.4261196602377
19:04 bilby INFO    : f021=607.6775384652
19:04 bilby INFO    : f022=367.7671211299739
19:04 bilby INFO    : f023=244.391377738396
19:04 bilby INFO    : f024=186.4940783438289
19:04 bilby INFO    : f025=465.135238339217
19:04 bilby INFO    : f026=339.3156871298895
19:04 bilby INFO    : f027=200.658805294298
19:04 bilby INFO    : f028=216.171230573016
19:04 bilby INFO    : f029=130.78951613875947
19:04 bilby INFO    : f030=263.9807142255208
19:04 bilby INFO    : f031=641.928222127829
19:04 bilby INFO    : f032=192.8565208431179
19:04 bilby INFO    : f033=315.443563681749
19:04 bilby INFO    : f034=163.0479129097612
19:04 bilby INFO    : f035=191.4509120380344
19:04 bilby INFO    : f036=345.2781364654948
19:04 bilby INFO    : f037=168.0966779772235
19:04 bilby INFO    : f038=420.189443214863
19:04 bilby INFO    : f039=62.29588783082522
19:04 bilby INFO    : f040=320.5922923290326
19:04 bilby INFO    : f041=335.8162133708968
19:04 bilby INFO    : f042=279.5965821510426
19:04 bilby INFO    : f043=275.7078326240928
19:04 bilby INFO    : f044=192.5919636477142
19:04 bilby INFO    : f045=290.25460815391
19:04 bilby INFO    : f046=207.96816335831227
19:04 bilby INFO    : fdot0=-1.227834e-15
19:04 bilby INFO    : fdot1=-4.297e-16
19:04 bilby INFO    : fdot2=-6.4737e-16
19:04 bilby INFO    : fdot3=-1.022991e-15
19:04 bilby INFO    : fdot4=-4.1895e-16
19:04 bilby INFO    : fdot5=-6.2782e-17
19:04 bilby INFO    : fdot6=-1.46389e-15
19:04 bilby INFO    : fdot7=-1.6868e-16
19:04 bilby INFO    : fdot8=-6.2003e-16
19:04 bilby INFO    : fdot9=-6.96131e-16
19:04 bilby INFO    : fdot10=-3.9329e-16
19:04 bilby INFO    : fdot11=-3.474e-16
19:04 bilby INFO    : fdot12=-3.80975e-16
19:04 bilby INFO    : fdot13=-7.33846e-16
19:04 bilby INFO    : fdot14=-9.69443e-16
19:04 bilby INFO    : fdot15=-2.81542e-16
19:04 bilby INFO    : fdot16=-8.64392e-16
19:04 bilby INFO    : fdot17=-4.08387e-16
19:04 bilby INFO    : fdot18=-7.04774e-16
19:04 bilby INFO    : fdot19=-2.151992e-15
19:04 bilby INFO    : fdot20=-5.38155e-16
19:04 bilby INFO    : fdot21=-4.853e-15
19:04 bilby INFO    : fdot22=-1.11907e-15
19:04 bilby INFO    : fdot23=-5.2065e-16
19:04 bilby INFO    : fdot24=-6.20476e-16
19:04 bilby INFO    : fdot25=-4.0719e-15
19:04 bilby INFO    : fdot26=-1.614814e-15
19:04 bilby INFO    : fdot27=-3.89748e-16
19:04 bilby INFO    : fdot28=-7.90875e-16
19:04 bilby INFO    : fdot29=-4.39475e-16
19:04 bilby INFO    : fdot30=-6.65848e-16
19:04 bilby INFO    : fdot31=-4.330987e-14
19:04 bilby INFO    : fdot32=-6.44814e-16
19:04 bilby INFO    : fdot33=-3.148e-16
19:04 bilby INFO    : fdot34=-7.9062e-16
19:04 bilby INFO    : fdot35=-1.76854e-16
19:04 bilby INFO    : fdot36=-9.5291e-16
19:04 bilby INFO    : fdot37=-3.1496e-16
19:04 bilby INFO    : fdot38=-9.25931e-16
19:04 bilby INFO    : fdot39=-1.156186e-16
19:04 bilby INFO    : fdot40=-1.51379e-15
19:04 bilby INFO    : fdot41=-1.7183e-16
19:04 bilby INFO    : fdot42=-9.392e-16
19:04 bilby INFO    : fdot43=-1.52788e-15
19:04 bilby INFO    : fdot44=-5.1439e-16
19:04 bilby INFO    : fdot45=-2.0477e-16
19:04 bilby INFO    : fdot46=-4.178455630932531e-16
19:04 bilby INFO    : distance0=181816860005.41092
19:04 bilby INFO    : distance1=32362224272.807774
19:04 bilby INFO    : distance2=160300000000.0
19:04 bilby INFO    : distance3=95000000000.0
19:04 bilby INFO    : distance4=71428052144.98288
19:04 bilby INFO    : distance5=121950332930.45859
19:04 bilby INFO    : distance6=115000000000.0
19:04 bilby INFO    : distance7=372200000000.0
19:04 bilby INFO    : distance8=70000000000.0
19:04 bilby INFO    : distance9=122000000000.0
19:04 bilby INFO    : distance10=90300000000.0
19:04 bilby INFO    : distance11=126900000000.0
19:04 bilby INFO    : distance12=68400000000.0
19:04 bilby INFO    : distance13=188677873590.52078
19:04 bilby INFO    : distance14=70000000000.0
19:04 bilby INFO    : distance15=151514050004.5091
19:04 bilby INFO    : distance16=74000000000.0
19:04 bilby INFO    : distance17=131060646137.58327
19:04 bilby INFO    : distance18=147057754416.14117
19:04 bilby INFO    : distance19=166665455004.96002
19:04 bilby INFO    : distance20=39500000000.0
19:04 bilby INFO    : distance21=714900000000.0
19:04 bilby INFO    : distance22=208331818756.20007
19:04 bilby INFO    : distance23=208331818756.20007
19:04 bilby INFO    : distance24=120000000000.0
19:04 bilby INFO    : distance25=700000000000.0
19:04 bilby INFO    : distance26=114000000000.0
19:04 bilby INFO    : distance27=149600000000.0
19:04 bilby INFO    : distance28=136500000000.0
19:04 bilby INFO    : distance29=111110303336.64005
19:04 bilby INFO    : distance30=120100000000.0
19:04 bilby INFO    : distance31=350000000000.0
19:04 bilby INFO    : distance32=121800000000.0
19:04 bilby INFO    : distance33=694000000000.0
19:04 bilby INFO    : distance34=630400000000.0
19:04 bilby INFO    : distance35=243900665860.9172
19:04 bilby INFO    : distance36=139900000000.0
19:04 bilby INFO    : distance37=174000000000.0
19:04 bilby INFO    : distance38=138887879170.80002
19:04 bilby INFO    : distance39=71428052144.98288
19:04 bilby INFO    : distance40=60000000000.0
19:04 bilby INFO    : distance41=180000000000.0
19:04 bilby INFO    : distance42=97086672818.42332
19:04 bilby INFO    : distance43=158700000000.0
19:04 bilby INFO    : distance44=86300000000.0
19:04 bilby INFO    : distance45=166665455004.96002
19:04 bilby INFO    : distance46=101099999999.99998
19:04 bilby INFO    : gamma0=1e-13
19:04 bilby INFO    : gamma1=1e-13
19:04 bilby INFO    : gamma2=1e-13
19:04 bilby INFO    : gamma3=1e-13
19:04 bilby INFO    : gamma4=1e-13
19:04 bilby INFO    : gamma5=1e-13
19:04 bilby INFO    : gamma6=1e-13
19:04 bilby INFO    : gamma7=1e-13
19:04 bilby INFO    : gamma8=1e-13
19:04 bilby INFO    : gamma9=1e-13
19:04 bilby INFO    : gamma10=1e-13
19:04 bilby INFO    : gamma11=1e-13
19:04 bilby INFO    : gamma12=1e-13
19:04 bilby INFO    : gamma13=1e-13
19:04 bilby INFO    : gamma14=1e-13
19:04 bilby INFO    : gamma15=1e-13
19:04 bilby INFO    : gamma16=1e-13
19:04 bilby INFO    : gamma17=1e-13
19:04 bilby INFO    : gamma18=1e-13
19:04 bilby INFO    : gamma19=1e-13
19:04 bilby INFO    : gamma20=1e-13
19:04 bilby INFO    : gamma21=1e-13
19:04 bilby INFO    : gamma22=1e-13
19:04 bilby INFO    : gamma23=1e-13
19:04 bilby INFO    : gamma24=1e-13
19:04 bilby INFO    : gamma25=1e-13
19:04 bilby INFO    : gamma26=1e-13
19:04 bilby INFO    : gamma27=1e-13
19:04 bilby INFO    : gamma28=1e-13
19:04 bilby INFO    : gamma29=1e-13
19:04 bilby INFO    : gamma30=1e-13
19:04 bilby INFO    : gamma31=1e-13
19:04 bilby INFO    : gamma32=1e-13
19:04 bilby INFO    : gamma33=1e-13
19:04 bilby INFO    : gamma34=1e-13
19:04 bilby INFO    : gamma35=1e-13
19:04 bilby INFO    : gamma36=1e-13
19:04 bilby INFO    : gamma37=1e-13
19:04 bilby INFO    : gamma38=1e-13
19:04 bilby INFO    : gamma39=1e-13
19:04 bilby INFO    : gamma40=1e-13
19:04 bilby INFO    : gamma41=1e-13
19:04 bilby INFO    : gamma42=1e-13
19:04 bilby INFO    : gamma43=1e-13
19:04 bilby INFO    : gamma44=1e-13
19:04 bilby INFO    : gamma45=1e-13
19:04 bilby INFO    : gamma46=1e-13
19:04 bilby INFO    : sigma_p=0.001
19:04 bilby INFO    : sigma_m=1e-10
19:04 bilby INFO    : Analysis likelihood class: <class 'bilby_wrapper.BilbyLikelihood'>
19:04 bilby INFO    : Analysis likelihood noise evidence: nan
19:04 bilby INFO    : Single likelihood evaluation took 4.984e-03 s
19:04 bilby INFO    : Using sampler Dynesty with kwargs {'nlive': 100, 'bound': 'live', 'sample': 'auto', 'periodic': None, 'reflective': None, 'update_interval': 600, 'first_update': None, 'npdim': None, 'rstate': None, 'queue_size': 32, 'pool': None, 'use_pool': None, 'live_points': None, 'logl_args': None, 'logl_kwargs': None, 'ptform_args': None, 'ptform_kwargs': None, 'gradient': None, 'grad_args': None, 'grad_kwargs': None, 'compute_jac': False, 'enlarge': None, 'bootstrap': None, 'walks': 100, 'facc': 0.2, 'slices': None, 'fmove': 0.9, 'max_move': 100, 'update_func': None, 'ncdim': None, 'blob': False, 'save_history': False, 'history_filename': None, 'maxiter': None, 'maxcall': None, 'dlogz': 20.0, 'logl_max': inf, 'n_effective': None, 'add_live': True, 'print_progress': True, 'print_func': <bound method Dynesty._print_func of <bilby.core.sampler.dynesty.Dynesty object at 0x1519b9588d90>>, 'save_bounds': False, 'checkpoint_file': None, 'checkpoint_every': 60, 'resume': False}
19:04 bilby INFO    : Checkpoint every check_point_delta_t = 600s
19:04 bilby INFO    : Using dynesty version 2.1.0
19:04 bilby INFO    : Live-point based bound method requested with dynesty sample 'auto', overwriting to 'multi'
19:04 bilby INFO    : Setting up multiproccesing pool with 32 processes
Ideal likelihood =  -179.5476962479997
RUN THE SAMPLER
19:04 bilby INFO    : Resume file ../data/nested_sampling//likelihood_expt_ozstar_resume.pickle does not exist.
19:04 bilby INFO    : Generating initial points from the prior
1it [00:00, ?it/s]1it [00:03, ?it/s, bound:0 nc:  1 ncall:1.0e+02 eff:1.0% logz=-inf+/-nan dlogz:inf>20]29it [00:03, 149.76it/s, bound:0 nc:  1 ncall:1.3e+02 eff:21.8% logz=-133659.99+/-0.24 dlogz:171913.602>20]52it [00:03, 136.34it/s, bound:0 nc:  1 ncall:1.6e+02 eff:31.5% logz=-5734.62+/-0.24 dlogz:3978.088>20]    72it [00:03, 124.21it/s, bound:0 nc:  3 ncall:2.0e+02 eff:36.4% logz=-2872.51+/-0.24 dlogz:1009.122>20]92it [00:04, 115.50it/s, bound:0 nc:  4 ncall:2.3e+02 eff:39.8% logz=-2172.00+/-0.24 dlogz:257.274>20] 107it [00:04, 104.79it/s, bound:0 nc:  3 ncall:2.6e+02 eff:40.8% logz=-2031.54+/-0.25 dlogz:127.905>20]117it [00:04, 87.69it/s, bound:0 nc:  2 ncall:2.9e+02 eff:39.8% logz=-1972.65+/-0.25 dlogz:61.347>20]  129it [00:04, 80.94it/s, bound:0 nc: 10 ncall:3.3e+02 eff:38.6% logz=-1949.91+/-0.23 dlogz:33.202>20]19:04 bilby INFO    : Written checkpoint file ../data/nested_sampling//likelihood_expt_ozstar_resume.pickle
19:04 bilby INFO    : Starting to close worker pool.
19:04 bilby INFO    : Finished closing worker pool.
140it [00:04, 28.26it/s, bound:0 nc:  1 ncall:4.6e+02 eff:67.4% logz=-1918.68+/-nan dlogz:0.023>20]  19:04 bilby INFO    : Rejection sampling nested samples to obtain 45 posterior samples
19:04 bilby INFO    : Sampling time: 0:00:04.758992
<frozen importlib._bootstrap>:228: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 96 from PyObject
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1374: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  posterior[key] = priors[key].peak
/fred/oz022/tkimpson/conda_envs/OzStar/lib/python3.9/site-packages/bilby/core/result.py:1401: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  data_frame['log_likelihood'] = getattr(
19:04 bilby INFO    : Summary of results:
nsamples: 45
ln_noise_evidence:    nan
ln_evidence: -1918.682 +/-  0.427
ln_bayes_factor:    nan +/-  0.427



