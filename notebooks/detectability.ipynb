{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detectability \n",
    "\n",
    "\n",
    "We are interested in how detectable a GW is using PTA + state space method\n",
    "\n",
    "We can use our state-space tools  to try to solve the problem of GW detection with a PTA i.e. _\"Is there evidence of a GW in my data?_. \n",
    "\n",
    "We can frame this as a model selection procedure where we have two models/hypotheses:\n",
    "\n",
    "\n",
    "* **Null Model** $M_0$. There is no GW in the data. In this case the measurement model of the Kalman filter simply returns the frequency states i.e. $g(t,\\theta)= 0$\n",
    "* **Alternative model** $M_1$. There is a GW in the data. The measurement model uses the full expression for $g(t,\\theta)$\n",
    "\n",
    "\n",
    "In order to accept the alternative hypothesis $M_1$ over $M_0$ there are two approaches we could take:\n",
    "\n",
    "1. The first is a fully Bayesian search over all the parameters for each model, calculating the evidence for each model and then determining the Bayes ratio. This is perhaps the most consistent way, but it is obviously expensive and at this stage we are keen to explore how detectability varies with e.g. GW strain.\n",
    "\n",
    "\t\n",
    "2. The second method is to recognize that $M_0$ and $M_1$ are hierarchically nested models and we can perform a likelihood ratio test. That is, given the maximum likelihood estimators $\\hat{\\theta}$ of the true parameters $\\theta$, the likelihood of each model can be calculate. These likelihoods are just  point estimates of the Bayes factor numerator/denominators. They can then be compared via the likelihood ratio $\\Lambda$. \n",
    "\n",
    "\t\n",
    "Given the cheap cost we proceed with the second method.\n",
    "\n",
    "\n",
    "### Likelihood ratio test\n",
    "For the likelihood ratio test we do not perform any kind of maximum likelihood search over the parameters for each of the models. Instead we just artificially set the maximum likelihood estimators to be equal to the true parameters of the system i.e. $\\hat{\\theta} = \\theta$. We assume that any maximum likelihood algorithm would converge to these parameters. This is obviously an oversimplification but will serve our purposes for now.\n",
    "\n",
    "\n",
    "Interpreting the likelihood ratio $\\Lambda$ also needs some consideration, since we have to account for the increased model complexity of $M_1$. Bayes factors penalise complexity by construction since one must integrate over a larger parameter space. \n",
    "\n",
    "There are many different ways to do this:\n",
    "\n",
    "* AIC\n",
    "* BIC\n",
    "* Wilks Theorem\n",
    "* ...\n",
    "\n",
    "\n",
    "In practice we will use different criteria to check they agree.\n",
    "\n",
    "Lets quickly review the number of parameters in each model:\n",
    "\n",
    "* The null model solely has parameters that correspond to the pulsars. $f_0$, $\\dot{f}$, $\\gamma$, $d$\n",
    "\n",
    "* The null model has all the pulsar parameters, plus 7 additional GW parameters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For now we will treat $sigma_p$ as a single paramter that is shared across $N$ pulsars\n",
    "\n",
    "\n",
    "\n",
    "This can be accomplished via Wilks' Theorem which states that for a large nuber of samples \\footnote{What counts as large? See \\url{https://www.osti.gov/servlets/purl/1529145}} the distribution of the test statistic approaches the chi-squared distribution under the null hypothesis i.e. \n",
    "\\begin{equation}\n",
    "2 \\log \\Lambda \\rightarrow \\chi^2\n",
    "\\end{equation}\n",
    "One can then compute $p$-values where the number of degrees of freedom is equal to the difference in the number of parameters of the two models; $M_1$ has 7 extra parameters over $M_0$\n",
    "\n",
    "\\begin{itemize}\n",
    "\t\\item Parameters $M_0$: $\\gamma$, $n$\n",
    "\t\\item Parameters $M_1$: $\\gamma$, $n$, $\\omega$, $\\theta$, $\\phi$, $\\psi$, $\\iota$, $\\Phi_0$,$h$\n",
    "\\end{itemize}\n",
    "With 7 degrees of freedom and a target tolerance of 5(1) \\% the test statistic is $\\sim 14 (18.5)$. For the toy system exhibited in Figs. \\ref{fig:model  example}, \\ref{fig:null example}, the test statistic is $\\gg 20$ - this makes intuitive sense since it is immediately obvious to the eye that the data is sinusoidal. But what about other systems? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
